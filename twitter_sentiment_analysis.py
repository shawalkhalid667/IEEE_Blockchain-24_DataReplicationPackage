# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LGyTEkiEcyIfPJd2yXf9mxIFd_OJGJHF

# Description
#### A sentiment analysis program for parsing tweets fetched from Twitter using Tweepy API and Python

# Get the Twitter Access Credentials
"""

# Import liberies
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
# Regular Expression
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# Add Twitter API credentials
bearer_token = ''
consumer_key = ''
consumer_secret = ''
access_token = ''
access_token_secret = ''

import tweepy

# Create the authentication object
authenticate = tweepy.OAuthHandler(consumer_key, consumer_secret)

# Set the access token and access token secret
authenticate.set_access_token(access_token, access_token_secret)

# Create the API object through passing in the authenticate info
api = tweepy.API(authenticate, wait_on_rate_limit = True)

# Create a Client object

# You can authenticate as your app with just your bearer token
# client = tweepy.Client(bearer_token)

# You can provide the consumer key and secret with the access token and access
# token secret to authenticate as a user

# client = tweepy.Client(
    # consumer_key=consumer_key, consumer_secret=consumer_secret,
    # access_token=access_token, access_token_secret=access_token_secret
# )



"""# Combine cvs files"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive/') 
# change directory to your folder 
# %cd "/content/gdrive/MyDrive/SEResearchCrypto/TwitterData"

import ast
import csv
temp_file_name='test2'
csv_file = open('./'+temp_file_name+'.csv', 'w')
temp_writer = csv.writer(csv_file)
header = ['Content', 'Number']
temp_writer.writerow(header)

for index in range(4):
  temp_row=["ABC", index]
  temp_writer.writerow(temp_row)

csv_file.close()

import pandas as pd
df_append = pd.DataFrame()
temp1 = pd.read_csv("80_1_23311.csv")
print(len(temp1))
df_append = df_append.append(temp1, ignore_index=True)
temp2 = pd.read_csv("80_2_2169.csv")
print(len(temp2))
df_append = df_append.append(temp2, ignore_index=True)
temp3 = pd.read_csv("80_3_1756.csv")
print(len(temp3))
df_append = df_append.append(temp3, ignore_index=True)

df_append
df_append.to_csv('80_final.csv',index=False)
print(len(df_append))

"""# Get tweet status with tweet IDs

##### - Get day within second 3 days
##### - Store in DataFrame with columns: created_at, text, number of likes 
##### - Data Cleaning
##### - Data Sentiment Analysis
##### - Adds more colunms: positive, neutral, negative
##### - Count Number of positive, neutral, and negative
##### - Output the result to a file
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import files
# uploaded = files.upload()

from google.colab import drive
drive.mount('/content/gdrive/') 
# change directory to your folder 
# %cd "/content/gdrive/MyDrive/SEResearchCrypto/TwitterData"

# 15_1392602041025843203_Day12_116871_210512_210514_100600
file_name = '15_1392602041025843203_Day12_116871_210512_210514_100600'

import ast
import csv

# open the file in the write mode
# print(file_name)
csv_f = open('./'+file_name+'.csv', 'w')
# create the csv writer
writer = csv.writer(csv_f)
# write a row to the csv file
header = ['Tweet Content', 'Created Date', 'Number of Like']
writer.writerow(header)

with open(file_name+'.txt', 'r') as f:
    tweetslist = ast.literal_eval(f.read())

# ~30 sec to get the 4190/4187 replies ID ran by pre-written script

# extract_thread_replies zouxiaojie$ python3 replies.py -t 1538406040374595585 -s 2022-06-20T01:30:00Z -e 2022-06-21T01:32:00Z
# 55 mins 15 s for 4183 tweets' text

nonen_count = 0
en_count = 0
for t in tweetslist:
    # print('TweetID ', t)
    try:
      get_tweet_status = api.get_status(t)
      # if get_tweet_status.lang=='en':
      # print(index_count, ': ', get_tweet_status.created_at, ' ', get_tweet_status.text)
      # index_count+=1

      # Write the data into the csv file
      if get_tweet_status.lang == 'en':
        temp_row = []
        temp_row.append(get_tweet_status.text)
        temp_row.append(get_tweet_status.created_at)
        temp_row.append(get_tweet_status.favorite_count)
        writer.writerow(temp_row)
        en_count+=1
        print("Number of en: ", en_count)
      else:
        # non-en count
        nonen_count+=1
        print("Number of non-en: ", nonen_count)
    except:
      pass
    
# close the file
csv_f.close()
# print("Non-en counts is: ", index_count)

"""# Begin the sentiment analysis"""

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import files
# uploaded = files.upload()

from google.colab import drive
drive.mount('/content/gdrive/') 
# change directory to your folder 
# %cd "/content/gdrive/MyDrive/SEResearchCrypto/TwitterData"

data_file_name = '15_1392602041025843203_Day12_116871_210512_210514_100600'
result_file_name = '15_result'

df = pd.read_csv (data_file_name + '.csv')
df['Tweet Content'] = df['Tweet Content'].astype('string')
df = df.dropna(subset=['Tweet Content'])
df = df.reset_index(drop=True)
df
#df['Commit Message'] = df['Commit Message'].astype(pd.StringDtype())
#print(df.info())

"""### Data Cleaning """

from IPython.display import HTML
import re
# Cleaning text using a help function

def cleanText(text):
  text = re.sub(r'@[A-Za-z0-9]+', '', text) #remove @mentions
  text = re.sub(r'#', '', text) #remove # symbol
  text = re.sub(r'\n', '', text) #remove next line
  text = re.sub(r'RT[\s]+', '', text) #remove RT
  text = re.sub(r'https?:\/\/\S+', '', text) #remove hyper link
  text = re.sub(r'[:_+-]', '', text) #remove unneccassary characters

  return text

def remove_emojis(data):
    emoj = re.compile("["
        u"\U00002700-\U000027BF"  # Dingbats
        u"\U0001F600-\U0001F64F"  # Emoticons
        u"\U00002600-\U000026FF"  # Miscellaneous Symbols
        u"\U0001F300-\U0001F5FF"  # Miscellaneous Symbols And Pictographs
        u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
        u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
        u"\U0001F680-\U0001F6FF"  # Transport and Map Symbols

        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
        "]+", re.UNICODE)
    return re.sub(emoj, '', data)

df['Tweet Content']=df['Tweet Content'].apply(cleanText)
df['Tweet Content']=df['Tweet Content'].apply(remove_emojis)
df

"""### Label the polarity with Positive and Negative"""

# Get the subjectivity and polarity of the tweets text

# TextBlob is a Python library for processing textual data. 
# It provides a consistent API for diving into common NLP tasks such as sentiment analysis, and more.

# Subjective: it tells how subjective or opinionated the text/data is
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

# Polarity: it tells how positive or negative the text/data is
def getPolarity(text):
  return TextBlob(text).sentiment.polarity

# Create subjectivity and polarity columns
df['Subjectivity'] = df['Tweet Content'].apply(getSubjectivity)
df['Polarity'] = df['Tweet Content'].apply(getPolarity)

# Show the dataframe with new columns
df

# A help function for compute the negative, netural and positive analysis

def getSentiment(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  elif score > 0:
    return 'Positive'


df['Analysis'] = df['Polarity'].apply(getSentiment)
df.to_csv(result_file_name + '.csv')
df

"""### Print all the positive and negative tweets separately"""

# Print all the positive tweets

j = 1
sortedDF = df.sort_values(by=['Polarity'])
print("All the Positive tweets \n")
# print(sortedDF.shape[0])

# Loop through the number of rows
for i in range(0, sortedDF.shape[0]):
  if (sortedDF['Analysis'][i] == 'Positive'):
    print(str(j) + '. ' + sortedDF['Tweet Content'][i] + '\n')
    j = j + 1

# Print all the Negative tweets

j = 1
sortedDF = df.sort_values(by=['Polarity'], ascending='False')
print("All the Negative tweets \n")

# Loop through the number of rows
for i in range(0, sortedDF.shape[0]):
  if (sortedDF['Analysis'][i] == 'Negative'):
    print(str(j) + '. ' + sortedDF['Tweet Content'][i] + '\n')
    j = j + 1

# Print all the neutral tweets

j = 1
sortedDF = df.sort_values(by=['Polarity'])
print("All the Positive tweets \n")
# print(sortedDF.shape[0])

# Loop through the number of rows
for i in range(0, sortedDF.shape[0]):
  if (sortedDF['Analysis'][i] == 'Neutral'):
    print(str(j) + '. ' + sortedDF['Tweet Content'][i] + '\n')
    j = j + 1

"""### Plot the Word Cloud"""

from pandas.core.common import random_state
# Plot the Word Cloud of the tweets

allWords = ' '.join( [twts for twts in df['Tweet Content']] )
wordCloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size= 119).generate(allWords)

plt.imshow(wordCloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Plot the counts"""

df['Analysis'].value_counts()

# Plot the visual counts
plt.title('Tweets Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
df['Analysis'].value_counts().plot(kind='barh')
nums = []
nums.append(df['Analysis'].value_counts()[0])
nums.append(df['Analysis'].value_counts()[1])
nums.append(df['Analysis'].value_counts()[2])
for index, value in enumerate(nums):
    plt.text(value, index,
             str(value))
plt.show()





"""# Get all replies from a specific tweet"""

# Examples Tweet from Elon Musk: 
# I will keep supporting Dogecoin (1538406040374595585)
# “That’s not punny!” (1567367045435609089)
# My tweets are being suppressed! (1569749150555594753)

user_name = 'elonmusk'
tweet_id = '1569749150555594753'
tweet_link = 'https://twitter.com/twitter/statuses/'+tweet_id

replies_likes = 10
replies_keyword = ''
queries = 'to:'+user_name+' '+'conversation_id:'+tweet_id 

print(tweet_link)

replies=[]
count = 1

# With the specified conversation_id, we can ONLY get the replies from most recent 7 days to that tweet

# for tweet in tweepy.Cursor(api.search, q=queries, lang='en', since_id=tweet_id, result_type='mixed', timeout=999999).items(1000):
for tweet in tweepy.Cursor(api.search, q=queries, lang='en', since_id=tweet_id, result_type='mixed', until='2022-09-14', timeout=999999).items(4200):
    print(count) # There will be 500 counts print when there is no conversation_id set up
    count+=1
    if hasattr(tweet, 'in_reply_to_status_id_str'):
        # and (tweet.favorite_count >= replies_likes) and (replies_keyword in tweet.text)
        #print(111)
        #print(tweet.in_reply_to_status_id_str)
        if (tweet.in_reply_to_status_id_str==tweet_id):
            #print(222)
            replies.append(tweet)


print('Size of the replies list: ', len(replies))

for i in range(len(replies)):
  print(replies[i].created_at, ' ', '# of likes: ', replies[i].favorite_count, ' ',replies[i].text)
  # print(replies[i].text)

tests = []
for tweet in tweepy.Cursor(api.search, q='from:elonmusk'+' '+'conversation_id:1567367045435609089', since_id='1567367045435609089', result_type='recent', timeout=999999).items(1000):
  tests.append(tweet)
print(len(tests))
for i in range(len(tests)):
  print(tests[i].created_at, ' ', tests[i].text)

"""# Get a specific user most recent posts

### Timestamp, number of likes
"""

# Extract 100 tweets from Twitter user - Elon Musk

# Time range, number of likes
# Search by tweet link to get the comments/replies

recentPosts = api.user_timeline(screen_name='elonmusk', count=1000, lang='en', include_rts = True, exclude_replies = False, tweet_mode='extended')
print(len(recentPosts))

# Print only 10 tweets from the account
i = 1
for tweet in recentPosts[0:5]:
  print(str(i) + '. ' + str(tweet.created_at) + ' ' + tweet.full_text + '\n')
  i = i + 1

# for status in tweepy.Cursor(api.search, q="from:elonmusk", rpp=100, show_user=True, fromDate=202208020000, toDate=202209010000, include_rts=False, exclude_replies=True).items(1000):
  # print(str(index) + '. ' + str(status.created_at) + ' ' + status.text)
  # index += 1
from re import search

createdAt =[]
likes = []
tweetText = []
links = []

# Request exceeds account’s current package request limits. 
# Please upgrade your package and retry or contact Twitter about enterprise access.

# ' conversation_id:1538406040374595585'

index = 1
for tweet in api.search_30_day(environment_name='development', query='from:elonmusk', fromDate=202209130000, toDate=202209140000):
  if search("@", tweet.text):
    pass
  else:
    createdAt.append(tweet.created_at)
    likes.append(tweet.favorite_count)
    tweetText.append(tweet.text)
    links.append('https://twitter.com/twitter/statuses/'+tweet.id_str)
    print(str(index) + '. ' + str(tweet.created_at) + ' ' + 'Number of Likes: ' + str(tweet.favorite_count) + ' ' + tweet.text)
    print(links[index-1])
    index += 1

df = pd.DataFrame()
df['Tweets']=tweetText
df['Create At'] = createdAt
df['Number of Likes']=likes
df['Tweet Link']=links

from IPython.display import HTML
# Cleaning text using a help function

def cleanText(text):
  text = re.sub(r'@[A-Za-z0-9]+', '', text) #remove @mentions
  text = re.sub(r'#', '', text) #remove # symbol
  text = re.sub(r'\n', '', text) #remove next line
  text = re.sub(r'RT[\s]+', '', text) #remove RT
  text = re.sub(r'https?:\/\/\S+', '', text) #remove hyper link
  text = re.sub(r'[:_+-]', '', text) #remove unneccassary characters

  return text

def make_clickable(val):
    return f'<a target="_blank" href="{val}">{val}</a>'

df.style.format({'Tweet Link': make_clickable})

df.head(40)

"""# Methods Testing """

# Using Cursor to interate a user account twitter
# Cursor makes pagination through an user twitters more easier
cursor_user = tweepy.Cursor(api.user_timeline, id='elonmusk', tweet_mode='extended').items(1)

for i in cursor_user:
  # Get all the informations containing in a single tweet
  print(dir(i), '\n')
  print('1.Author: ', i.author)
  print('2.Contributors: ', i.contributors)
  print('3.Create_at: ', i.created_at)

# Bitcoin hashtag
cursor_hashtag = tweepy.Cursor(api.search, q='Bitcoin', tweet_mode='extended', lang='en').items(6)

count = 1
for i in cursor_hashtag:
  print(count, ': ', i.full_text, '\n')
  count = count + 1

post = api.user_timeline(screen_name='elonmusk', count=5, lang='en', tweet_mode='extended', exlude_replies=True)
for tweet in post:
  print(tweet.full_text)
  print(tweet.id)

import sys
import time
import logging
import urllib.parse

from os import environ as e

# The search index has a 7-day limit. 
# In other words, no tweets will be found for a date older than one week.

name = 'elonmusk'
tweet_id = '1557943469984956417'

replies=[]
for tweet in tweepy.Cursor(api.search, q='to:'+name, since_id=tweet_id, result_type='popular', timeout=999999).items(10):
    if hasattr(tweet, 'in_reply_to_status_id_str'):
        print(111)
        print(tweet.in_reply_to_status_id_str)
        if (tweet.in_reply_to_status_id_str==tweet_id):
            print(222)
            replies.append(tweet)

print('Size of the replies list: ', len(replies))

for i in range(len(replies)):
  print(replies[i].text)

# Search tweets using keyword within a period of time
tweets_within_timeperiod = tweepy.Cursor(api.search,  
              q="Bitcoin",
              since="2022-08-10", 
              until="2022-08-11",
              lang='en').items(5)
count = 0
for i in tweets_within_timeperiod:
  print(count, ': ', i.text)
  count = count + 1

# Counting the number of hashtags

# Twitter api provides historical data for a hashtag only up to past 7-10 days.
# Running time is too long

search_words = "#bitcoin"
date_since = "2022-08-06"

tweets = tweepy.Cursor(api.search,
                  q=search_words,
                  lang="en",
                  fromDate=date_since).items(50)
                  
count = 0
for tweet in tweets:
  count = count + 1
  #print(tweet.text)
print(count)